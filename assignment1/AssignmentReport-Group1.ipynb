{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report Olof Ljunggren \n",
    "## TDT4265 Computer vision and deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"task1.jpg\" alt=\"Derivation for gradient in Logistic Regression.\" name=\"Derivation for gradient in Logistic Regression.\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2a)\n",
    "- Implementation of normalization, forward propagation, backward propagation and cross entropy loss was done here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "\n",
    "- Figure with training and validation loss:\n",
    "\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "\n",
    "<!-- - Figure with training and validation accuracy: -->\n",
    "\n",
    "![](task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "- The early stop kicked in at epoch 33. (Started counting at 0). This was done by checking if the lowest of the 10 recent values was the oldest. In that case the loss has not improved for last 10 values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "\n",
    "- Figure with shuffled vs unshuffled dataset. The spikes appears in the unshuffled set because it kind of lose generalization. \n",
    "There will probably be spikes when we get some data points from number 2 and some from set with number 3. \n",
    "Since the data is not shuffled the structure of the initial data matters.\n",
    "\n",
    "![](task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2a)\n",
    "- Implementation of one-hot encoding, forward propagation, backward propagation and multiple label cross entropy loss was done here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "\n",
    "- Figure with training and validation loss for the softmax regression problem:\n",
    "\n",
    "![](task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "\n",
    "- Figure with training and validation accuracy for the multilabel regression problem. \n",
    "\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "We see some signs of overfitting. This can be seen by looking at the diffrence between the both accuracies. \n",
    "Since the validation data is not improving as much as the training data we are probably overfitting the data to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "- Since derivation is associative the cost from the multi-class cross-entropy cost will stay the same but we will have an extra term. \n",
    "This new term will punish large weights. \n",
    "\n",
    "<img src=\"task4a.jpg\" alt=\"Derivation for gradient in l2 Softmax Regression.\" name=\"Derivation for gradient in l2 Softmax Regression.\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "- Figure with lambda = 0: \n",
    "\n",
    "<img src=\"task4b_softmax_weight.png\" alt=\"Final weights corresponding to lambda = 0.\" name=\"Final weights corresponding to lambda = 0.\" width=\"560\"/>\n",
    "\n",
    "- Figure with lambda = 1: \n",
    "\n",
    "<img src=\"task4b_softmax_weight_l2.png\" alt=\"Final weights corresponding to lambda = 1.\" name=\"Final weights corresponding to lambda = 1.\" width=\"560\"/>\n",
    "\n",
    "- It is less noisy weigths in the second case since the lambda = 1 penalize high weigths which corresponds to high frequenciers and noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "\n",
    "- Figure with validation loss for diffrent values for lambda. (lambda: 1.0, 0.1, 0.01, 0.001)\n",
    "\n",
    "![](task4c_softmax_l2_val_acc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "\n",
    "- The precision degrades with increasing lambda and it penalizes complex solutions. This gives generalization but of course simplifies models. I believe this simplification is the main reason for this.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "\n",
    "- Figure with penalizing factor lambda plotted against the Frobenius norm size of the trained weigths. As expected the value of the generalization constant lambda directly affects the size of the weigths.\n",
    "\n",
    "![](task4d_l2_reg_norms.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdt4265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
