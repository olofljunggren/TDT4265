{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Report Olof Ljunggren 2024-02-22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1a)\n",
    "\n",
    "<img src=\"Task1a.jpg\" alt=\"Derivation for weight update.\" name=\"Derivation for weight update.\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1b)\n",
    "\n",
    "<img src=\"Task1b.jpg\" alt=\"Vectorization for weight update.\" name=\"Vectorization for weight update.\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2a)\n",
    "A mean and standard deviation for the entire dataset used was calculated in the file normalization.py. There we found that mean = 33.55274553571429 and std = 78.87550070784701."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "Forward and backward functions were implemented. This was in a way that could manage multiple nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "<img src=\"task2c_train_loss.png\" alt=\"Train loss.\" name=\"Train loss.\" width=\"1500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "In total there should be 285 * 64 + 64 * 10 weights = 18880. We also saved results from intermediate results for backward propagation, this is 32 * 285 + 32 * 64, minibatch with 32 numbers and 285 inputs to hidden layer and 64 inputs to output layer. But as learnable parameters we have 18880."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "In this third task I implemented better weight initialization, an improved sigmoid and finally gradient momentum. \n",
    "\n",
    "1. Improved weight initialization: as we can see in the plot the gaussian distributed weights gave quite better results. We can notice that the validation seems to stabilize around 10000 training steps instead of still improving around 30000 as in task 2c. Also its seems like the weights converged to a little bit better result, more accurate classification. We reached 95% accuracy in ~5000 repetitions.\n",
    "\n",
    "2. Improved sigmoid: also here the convergence speed seemed to improved quite significantly. Now we reached 95% accuracy in ~2000 repetitions compared to ~5000 repetitions in the previous subtask. Also here the final accuracy seems to be a little bit better.\n",
    "\n",
    "3. Momentum: the momentum further more improvec the convergence speed. Now we reached 95% accuracy in a little less than ~2000 repetitions.Here the final accuracy does not seem to improve. Almost the opposite, which kind of makes sense since we are not only using the correct gradient. This also gives a jumpy behavior. Though, in total the convergence were faster.\n",
    "\n",
    "Finally all of these 3 we still have tendecies of overfitting. This is seen by looking at the large diffrence between training and validation accuracy. The reason for this is probably the number of nodes in the hidden layer in this case. But it is hard to draw conclusion now when we have not tested any other network topologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3a)\n",
    "<img src=\"task3a.png\" alt=\"Train loss with improved weight init.\" name=\"Train loss with improved weight init.\" width=\"1500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "<img src=\"task3b.png\" alt=\"Train loss with improved sigmoid.\" name=\"Train loss with improved sigmoid.\" width=\"1500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "<img src=\"task3c.png\" alt=\"Train loss with momentum.\" name=\"Train loss with momentum.\" width=\"1500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "Using 32 nodes in the hidden layers gives worse result than for 64. We loose some accuracy.\n",
    "\n",
    "<img src=\"task4a.png\" alt=\"Comparison 32 vs 64 nodes in the hidden layer.\" name=\"Comparison 32 vs 64 nodes in the hidden layer.\" width=\"1500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "Using 128 nodes in the hidden layers isntead of 64 does improve a little, but not as much as from 32 to 64. Hence, we have probably overfit the network a little bit.\n",
    "\n",
    "<img src=\"task4b.png\" alt=\"Comparison 64 vs 128 nodes in the hidden layer.\" name=\"Comparison 64 vs 128 nodes in the hidden layer.\" width=\"1500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "By using to equal size hidden layers with size 54 nodes gives 285 * 54 + 54 * 54 + 54 * 10 = 18846 which is a little fewer parameters than the task 3 case with 18880 parameters for one hidden layer with 64 nodes. In this case it seems like the results of the two networks are quite similar. Both converge to ~96% accuracy after ~8000 training steps.\n",
    "\n",
    "<img src=\"task4d.png\" alt=\"Two hidden layers with 54 nodes.\" name=\"Two hidden layers with 54 nodes.\" width=\"1500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "In this case we have a really severe case of overfitting. We fit the model alomst perfect onto our training data but we lose the generalization and therefore the performance on the validation set.\n",
    "\n",
    "<img src=\"task4e.png\" alt=\"Two hidden layers with 54 nodes vs ten layers with 64 nodes.\" name=\"Two hidden layers with 54 nodes vs ten layers with 64 nodes.\" width=\"1500\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
